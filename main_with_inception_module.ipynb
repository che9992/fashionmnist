{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main_with_inception_module <hr/>\n",
    "\n",
    "\n",
    "### How looks Google Inception Module ?\n",
    "<img src=\"./inception_implement.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\che99\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and cut datas\n",
    "train_data = pd.read_csv('fashion-mnist_train.csv', dtype='float32')\n",
    "train_data = np.array(train_data)\n",
    "train_Y = train_data[:,[0]]\n",
    "train_X = train_data[:,1:]\n",
    "\n",
    "test_data = pd.read_csv('fashion-mnist_test.csv', dtype='float32')\n",
    "test_data = np.array(test_data)\n",
    "test_Y = test_data[:,[0]]\n",
    "test_X = test_data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator():\n",
    "    where = 0\n",
    "    def __init__(self, x, y, batch_size, one_hot = False, nb_classes = 0):\n",
    "        self.nb_classes = nb_classes\n",
    "        self.one_hot = one_hot\n",
    "        self.x_ = x\n",
    "        self.y_ = y\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.total_batch = int(len(x) / batch_size)\n",
    "        self.x = self.x_[:batch_size,:]\n",
    "        self.y = self.y_[:batch_size,:]\n",
    "        self.where = batch_size\n",
    "        \n",
    "        if self.one_hot :\n",
    "            self.set_one_hot()\n",
    "\n",
    "    def next_batch(self):\n",
    "        if self.where + self.batch_size > len(self.x_) :\n",
    "            self.where = 0\n",
    "            \n",
    "        self.x = self.x_[self.where:self.where+self.batch_size,:]\n",
    "        self.y = self.y_[self.where:self.where+self.batch_size,:]\n",
    "        self.where += self.batch_size\n",
    "        \n",
    "        if self.one_hot:\n",
    "            self.set_one_hot()\n",
    "        \n",
    "    def set_one_hot(self):\n",
    "        self.y = np.int32(self.y)\n",
    "        one_hot = np.array(self.y).reshape(-1)\n",
    "        self.y = np.eye(self.nb_classes)[one_hot].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=([None, 784]), dtype=tf.float32)\n",
    "X_img = tf.reshape(X, shape=[-1,28,28,1])\n",
    "Y = tf.placeholder(shape=([None, 10]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weight(size, name):\n",
    "    return tf.Variable(tf.random_normal(size, stddev=0.01), name=name)\n",
    "\n",
    "def create_bias(size,name):\n",
    "    return tf.Variable(tf.random_normal(size, stddev=0.01), name=name)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pooling(x, size):\n",
    "    return tf.nn.max_pool(x, ksize=size, strides=[1,1,1,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "map1 = 32\n",
    "map2 = 64\n",
    "num_fc1 = 700\n",
    "num_fc2 = 10\n",
    "reduce1x1 = 16\n",
    "dropout = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inception module1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# followings input\n",
    "W_1x1_conv1_1 = create_weight([1,1,1,reduce1x1], 'W_1x1_conv1_1')\n",
    "b_1x1_conv1_1 = create_bias([reduce1x1], 'b_1x1_conv1_1')\n",
    "\n",
    "W_1x1_conv1_2 = create_weight([1,1,1,reduce1x1], 'W_1x1_conv1_2')\n",
    "b_1x1_conv1_2 = create_bias([reduce1x1], 'b_1x1_conv1_2')\n",
    "\n",
    "W_1x1_conv1_3 = create_weight([1,1,1,map1], 'W_1x1_conv1_3')\n",
    "b_1x1_conv1_3 = create_bias([map1], 'b_1x1_conv1_3')\n",
    "\n",
    "# followings 1x1\n",
    "W_3x3_conv1 = create_weight([3,3,reduce1x1,map1], 'W_3x3_conv1')\n",
    "b_3x3_conv1 = create_bias([map1], 'b_3x3_conv1')\n",
    "\n",
    "W_5x5_conv1 = create_weight([5,5,reduce1x1,map1], 'W_5x5_conv1')\n",
    "b_5x5_conv1 = create_bias([map1], 'b_5x5_conv1')\n",
    "\n",
    "\n",
    "# followings max pooling\n",
    "W_1x1_conv1_4 = create_weight([1,1,1,map1], 'W_1x1_conv1_4')\n",
    "b_1x1_conv1_4 = create_bias([map1], 'b_1x1_conv1_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inception module2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# followings Inception1\n",
    "W_1x1_conv2_1 = create_weight([1,1,4*map1,reduce1x1], 'W_1x1_conv2_1')\n",
    "b_1x1_conv2_1 = create_bias([reduce1x1], 'b_1x1_conv2_1')\n",
    "\n",
    "W_1x1_conv2_2 = create_weight([1,1,4*map1,reduce1x1], 'W_1x1_conv2_2')\n",
    "b_1x1_conv2_2 = create_bias([reduce1x1], 'b_1x1_conv2_2')\n",
    "\n",
    "W_1x1_conv2_3 = create_weight([1,1,4*map1,map2], 'W_1x1_conv2_3')\n",
    "b_1x1_conv2_3 = create_bias([map2], 'b_1x1_conv2_3')\n",
    "\n",
    "\n",
    "# followings 1x1\n",
    "W_3x3_conv2 = create_weight([3,3,reduce1x1,map2], 'W_3x3_conv2')\n",
    "b_3x3_conv2 = create_bias([map2], 'b_3x3_conv2')\n",
    "\n",
    "W_5x5_conv2 = create_weight([5,5,reduce1x1,map2], 'W_5x5_conv2')\n",
    "b_5x5_conv2 = create_bias([map2], 'b_5x5_conv2')\n",
    "\n",
    "\n",
    "# followings max pooling\n",
    "W_1x1_conv2_4 = create_weight([1,1,4*map1,map2], 'W_1x1_conv2_4')\n",
    "b_1x1_conv2_4 = create_bias([map2], 'b_1x1_conv2_4')\n",
    "\n",
    "#Fully connected layers\n",
    "W_fc1 = create_weight([28*28*(4*map2),num_fc1], 'W_fc1')\n",
    "b_fc1 = create_weight([num_fc1], 'b_fc1')\n",
    "\n",
    "W_fc2 = create_weight([num_fc1, num_fc2], 'W_fc2')\n",
    "b_fc2 = create_weight([num_fc2], 'b_fc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, train=True):\n",
    "    # inception module 1 \n",
    "    conv1_1x1_1 = tf.nn.relu(conv2d(X_img, W_1x1_conv1_1) + b_1x1_conv1_1)\n",
    "    conv1_1x1_2 = tf.nn.relu(conv2d(X_img, W_1x1_conv1_2) + b_1x1_conv1_2)\n",
    "    maxpool1 = max_pooling(X_img,[1,3,3,1])\n",
    "\n",
    "    conv1_3x3 = conv2d(conv1_1x1_1, W_3x3_conv1) + b_3x3_conv1\n",
    "    conv1_5x5 = conv2d(conv1_1x1_2, W_5x5_conv1) + b_5x5_conv1\n",
    "    conv1_1x1_3 = conv2d(X_img, W_1x1_conv1_3) + b_1x1_conv1_3\n",
    "    conv1_1x1_4 = conv2d(maxpool1, W_1x1_conv1_4) + b_1x1_conv1_4\n",
    "    \n",
    "    inception1 = tf.nn.relu(tf.concat([conv1_3x3, conv1_5x5, conv1_1x1_3, conv1_1x1_4], 3))\n",
    "\n",
    "    # inception module 2\n",
    "    conv2_1x1_1 = tf.nn.relu(conv2d(inception1, W_1x1_conv2_1) + b_1x1_conv2_1)\n",
    "    conv2_1x1_2 = tf.nn.relu(conv2d(inception1, W_1x1_conv2_2) + b_1x1_conv2_2)\n",
    "    maxpool2 = max_pooling(inception1,[1,3,3,1])\n",
    "\n",
    "    conv2_3x3 = conv2d(conv2_1x1_1, W_3x3_conv2) + b_3x3_conv2\n",
    "    conv2_5x5 = conv2d(conv2_1x1_2, W_5x5_conv2) + b_5x5_conv2\n",
    "    conv2_1x1_3 = conv2d(inception1, W_1x1_conv2_3) + b_1x1_conv2_3\n",
    "    conv2_1x1_4 = conv2d(maxpool2, W_1x1_conv2_4) + b_1x1_conv2_4\n",
    "    \n",
    "    inception2 = tf.nn.relu(tf.concat([conv2_3x3, conv2_5x5, conv2_1x1_3, conv2_1x1_4], 3))\n",
    "    \n",
    "    inception2_flat = tf.reshape(inception2, [-1, 28*28*4*map2])\n",
    "    \n",
    "    if train:\n",
    "        h_fc1 = tf.nn.dropout(tf.nn.relu(tf.matmul(inception2_flat, W_fc1) + b_fc1), dropout)\n",
    "        \n",
    "    else:\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(inception2_flat, W_fc1) + b_fc1)\n",
    "        \n",
    "    return tf.matmul(h_fc1, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = model(X), labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model(X,train=False), 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n",
      "epoch: 0 cost: 0.010257292232830275\n",
      "epoch: 1 cost: 0.009854835714704677\n",
      "epoch: 2 cost: 0.009519968060127811\n",
      "epoch: 3 cost: 0.007952607767698894\n",
      "epoch: 4 cost: 0.006007270039578239\n",
      "epoch: 5 cost: 0.006704278422463779\n",
      "epoch: 6 cost: 0.007705514641684205\n",
      "epoch: 7 cost: 0.006935674203112211\n",
      "epoch: 8 cost: 0.006242332361783463\n",
      "epoch: 9 cost: 0.004668588463887319\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "print('Started!')\n",
    "\n",
    "batch = BatchGenerator(batch_size=100, one_hot=True, nb_classes=10, x=train_X, y=train_Y)\n",
    "for i in range(epoch):\n",
    "    avg_cost = 0\n",
    "    for b in range(batch.total_batch):\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={X:batch.x, Y:batch.y})\n",
    "        batch.next_batch()\n",
    "        avg_cost += c / batch.total_batch\n",
    "    print('epoch:', i, 'cost:', avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- TRAIN DATA ACCURACY -----\n",
      "99.8583334684372 %\n",
      "----- TEST DATA ACCURACY -----\n",
      "92.59000039100647 %\n"
     ]
    }
   ],
   "source": [
    "print('----- TRAIN DATA ACCURACY -----')\n",
    "accu  = 0\n",
    "batch = BatchGenerator(train_X, train_Y, batch_size=100, nb_classes=10,one_hot=True)\n",
    "for i in range(batch.total_batch):\n",
    "    feed_dict = {X:batch.x, Y:batch.y}\n",
    "    accu += sess.run(accuracy, feed_dict=feed_dict)\n",
    "    batch.next_batch()\n",
    "\n",
    "print( (accu / batch.total_batch) * 100 , '%' )\n",
    "\n",
    "\n",
    "print('----- TEST DATA ACCURACY -----')\n",
    "accu  = 0\n",
    "test = BatchGenerator(test_X, test_Y, batch_size=100 ,one_hot=True, nb_classes=10)\n",
    "for i in range(test.total_batch):\n",
    "    feed_dict = {X:test.x, Y:test.y}\n",
    "    accu += sess.run(accuracy, feed_dict=feed_dict)\n",
    "    test.next_batch()\n",
    "\n",
    "print( (accu / test.total_batch) * 100 , '%' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
